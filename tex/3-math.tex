In this chapter, we present an overview of the needed concepts to a better understanding of this thesis. First, we describe the main properties of Polynomials over finite fields. After, we present an overview of the coding theory. We explain the main structures and operations. In the end, we present a fundamental code for this works, which is the Goppa Codes, and we present and Literature reviews which the main works on the root-finding algorithm in code-based cryptosystems. 

\section{Algebraic structures}
The goal of this works is to define a secure way to compute the roots of a polynomial over a binary finite field. In this section, we define it and present the main characteristics of it.
\begin{definition}
Let $\mathbb{F}_{2^m}[x]$ a binary finite field with size of $n=2^m$, the polynomial $f = \sum_{i=0}^{t}{g_{i}x^{i}}$ is a polynomial of degree $t$
\end{definition}
We also can define this polynomial as a product of $t$ polynomials
\begin{equation*}
    f = \prod_{i=0}^{t}{p}, p \in \mathbb{F}_{2^m}.
\end{equation*}
Given this idea of a polynomial it is composed by the multiplication of others polynomials, we can define an irreducible polynomial as follow.
\begin{definition}
Let $f = \sum_{i=0}^{t}{g_{i}x^{i}}$ be a polynomial of degree $t$, $f$ is a irreducible polynomial if $f$ can not be factored into the product of two or more non-constant polynomials.
\end{definition}
Also, we define an monic polynomial.
\begin{definition}
Let $f = \sum_{i=0}^{t}{g_{i}x^{i}}$ be a polynomial of degree $t$, $f$ is a monic polynomial if the leading coefficient is equals to 1. 
\end{definition}
Thus, a monic polynomial of degree $t$ has the form $x^t + g_{t-1}x^{t-1} + \dots + g_0$. Finally, we define the roots of a polynomial.
\begin{definition}
Let $f = \sum_{i=0}^{t}{g_{i}X^{i}}$ be a polynomial of degree $t$. The root of $f$ are all elements in $\mathbb{F}_{2^m}$ that the result of evaluation in $f$ is 0.
\end{definition}
We can note that a root of a polynomial it is equivalent to an factor of it. Since a polynomial it is composed by a multiplication of their factors, when we evaluate the polynomial with one factor, the corresponding factor on the product will be equals to zero. Consequently, the results of all evaluation are equals to zero.

\section{Coding theory}
Coding theory is an engineering area that focuses on data transmission. Since many communications channels are susceptible to the introduction of error, for example, wireless connection. The error detection and correction techniques able to us to recover the original sent message. The object of study of this works it is and cryptography scheme that made uses of an error correction code to achieve key exchange between two parts. In the following sections, we give a brief introduction of the main concepts of error correction codes, and we also present the Goppa code, the linear code used to achieve the key exchange.


\subsection{Linear codes}
\begin{definition}
Let $\mathbb{F}_{q}$ be a finite field with $q$ elements. The linear code $[n, k]\;C$ is a subspace of dimension $k$ of the vector space $\mathbb{F}_{q}^n$.
\end{definition}

The parameter $k$ is the code dimension, and $n$ represents the length. The code $C$ was able to correct up to $t$ errors. This recovery is given through the redundancy added in a message with size $k$, encoded in a codeword of size $n$. All messages in $\mathbb{F}_{q}^n$ are mapped to a unique codeword. One of the most used metrics in codes is the Hamming metrics, defined bellow.

\begin{definition}
Let $a = (a_1, a_2, \dots, a_n)$ and $b = (b_1, b_2, \dots, b_n) \; \in \; C$ be two codewords. The hamming distance between $a$ and $b$ is the number of positions in which they differ.
\end{definition}

\begin{definition}
Let $a = (a_1, a_2, \dots, a_n) \; \in \; C$ be a codeword. The Hamming weight of the codeword $a$ is the number of non-zeri positions.
\end{definition}


\subsection{Goppa codes}
Let $m, n, t\in \mathbb{N}$. A binary Goppa code $\Gamma(L, g(z))$ is defined by a polynomial $g(z) = \sum_{i=0}^{t}g_iz^i$ over $\mathbb{F}_{2^m}$ with degree $t$ and supported by $L = (\alpha_1, \alpha_2, \dots, \alpha_n)$, such that $\alpha_i \in \mathbb{F}_{2^m}$ with $\alpha_i \neq \alpha_j$ for $i\neq j$, such that $g(\alpha_i) \neq 0$ for all $\alpha_i \in L$ and $g(z)$ is squarefree. To a vector  $c = (c_1, \ldots, c_{n}) \in \mathbb{F}^n_{2}$ we associate a syndrome polynomial associate the syndrome polynomial
\begin{align}
  S_c(z) = \sum_{i=1}^{n} \frac{c_i}{z+\alpha_i},  
\end{align}
where ${z+\alpha_i}$ is invertible $\pmod{g(z)}$.
\begin{definition}
The binary Goppa code $\Gamma(L, g(z))$ consists of all vectors $c \in \mathbb{F}_{2}^n$ such that
\begin{equation}
    S_c(z) \equiv 0 \pmod{g(z)}.
\end{equation}
\end{definition}

% Change this
The parameters of a linear code are the size $n$, dimension $k$, and minimum distance $d$. We use the notation $[n,k,d]-$Goppa code for a binary Goppa code with parameters $n,k$ and $d$. If the polynomial $g(z)$ which defines a Goppa code is irreducible over $\mathbb{F}_{2^m}$, the code is an irreducible Goppa code.

The length of a Goppa code is given by $n = |L|$ and its dimension is $k \geq n-mt$, where $t = deg(g)$, and the minimum distance of $\Gamma(L, g(z))$ is $d \geq 2t + 1$. The syndrome polynomial $S_c(z)$ can be written as:
\begin{equation}
    S_c(z) \equiv \frac{w(z)}{\sigma(z)} \pmod{g(z)},
\end{equation}
where $\sigma(z) = {\displaystyle \prod_{i=1}^{l}(z+\alpha_i)}$ is the product over those $(z+\alpha_i)$ where there is an error in position $i$ of $c$. This polynomial $\sigma(z)$ is called Error-Locator Polynomial (ELP).

A binary Goppa code can correct a codeword $c \in \mathbb{F}_{2}^n$, obscured by an error vector $e \in \mathbb{F}_{2}^n$ with Hamming weight $w_h(e)$ up to $t$, i.e., the numbers of non-zero entries in $e$ is at most $t$. The way to correct errors is by using a decoding algorithm. For irreducible binary Goppa codes, we have three main alternatives for that. The extended Euclidean Algorithm (EEA) and the Berlekamp-Massey algorithm are out of the scope of this work because they needed a parity-check matrix that has twice more rows than columns. The Patterson algorithm~\cite{patterson1975algebraic}, the focus of our works, can correct up to $t$ errors with a smaller structure.

\section{Public-key cryptography}
Public-key cryptography, first introduced by Diffie and Hellman~\cite{diffie1976new} as a new concept to cryptography area. After that, Rivest, Shamir, and Adleman propose one of the most important public-key cryptosystems that still are massively used on different cryptography applications~\cite{rivest1978method}. The main idea of asymmetric cryptography was to associate a public key to a private key. The public key is distributed for the interested and used to encryption; Thus, given a ciphertext, only who knows the private key can recover the plain text. Using this idea, we present in the next section the idea of key encapsulation mechanisms.

\subsection{Key encapsulation mechanisms}
Most of the public-key algorithm is clumsy to use in data transmission. Thus, a key encapsulation mechanism could be used to exchange a symmetric key, and the data transmission could be done using an efficiency algorithm, like the AES~\cite{daemen2013design}. Key encapsulation mechanisms are widely used in many protocols, for example, the TLS (Transport Layer Security) protocol. It made uses of digital certificates to guarantee the authenticity of the server and provide a private connection between the browsers and servers.

A Key encapsulation mechanism is defined by a 3-tuple of algorithms \texttt{(KeyGen, Enc, Dec)}. Where \texttt{KeyGen} is a probabilistic algorithm that given a security parameter and outputs a key pair. The \texttt{Enc} is an encapsulation algorithm that receives as input a public key and a plain text and returns a ciphertext. The encapsulation process could be a probabilistic algorithm. The \texttt{Dec} is a decapsulation algorithm, which takes as input the ciphertext and the private key and returns the plaintext or a failure. In the Chapter~\ref{ch:code-based}, we present an implementation of a Key encapsulation mechanism.

\section{Side-channel attacks}
In summary, a side-channel attack is any attack when the attacker acquires sensitive information from the implementation of the cryptosystem. There are several types of side-channel attacks, and almost of then require extensive knowledge over the target cryptosystem. Even the standard cryptography primitives, like the RSA~\cite{rivest1978method}, are susceptible to side-channel attacks ~\cite{kocher1996timing}. 

Similar to the traditional cryptography, the quantum resistance cryptography is susceptible to most of the side-channel attacks. In the case of the code-based cryptosystems, one of the most dangerous side-channel attacks is based on the variance timing on the execution of the operations. This attack is called timing side-channel attack, and a brief idea is presented in~\ref{sub:timing-attack}

\subsection{Timing side-channel attacks}\label{sub:timing-attack}
A timing side-channel attack was an attack where it is possible to infer information from the time execution of a cryptography algorithm. To better understanding how a timing side-channel attack works, we present Example~\ref{example-timing}. 

\begin{example}\label{example-timing}
Let us suppose that an attacker has access to the execution time of a password validation (For instance, we use a four-digit password). This validation was made through the naive implementation presented in the Listings~\ref{lst:naive-pass}

\begin{lstlisting}[caption={Naive implementation of password check },label={lst:naive-pass},language=C]
//C
int passCheck(char * input_pass) {
    int i = 0;
    while (i < 4) {
        if (input_pass[i] == getPassAt(i)) {
            continue;
        } else {
            return -1;
        }
    }
    return 0;
}
\end{lstlisting}
So, an attacker with knowledge of the implementation, with access to the time wasted to verify a password, could perform a simple attack. First, flipping the first char to all possible chars, and assume that the correct was that with spent more time. This assumption works because, in the execution of the correct chat, the password verification will check the correct and on more char. After repeating this for all remaining characters, the attacker recovers the secret password.
\end{example}

We present this toy example to give an idea of how an attacker, who knows the implementation details can infer information about the execution time of an algorithm. In more complex cryptosystems, this inference was not simple. However, it still possible to steal sensitive information measuring the decoding execution time.

The authors in~\cite{shoufan2009timing} present the idea of inferring information on the time variance when we flip a bit over a ciphertext. This idea becomes of the fact that if we flip a correct bit (i. e. a position were an error was added), the algorithm will have fewer errors to correct and will return early. The opposite occurs when we flip the wrong bit. Thus, the algorithm will return lately. This attack was base of the attack presented in Chapter~\ref{ch:code-based}.


\section{Literature review}
The decoding process of the McEliece cryptosystem is the most sensitive part of the algorithm. The usage of the private key demands that this algorithm does not leak any information about the key, or even, any information about the error added to the message. The authors in \cite{bucerzan2017improved} show that a time deviation on the root-finding process in Patterson's decoding algorithm could open avenues for side-channel attacks. 

In this section, we present the related works as the results of our literature review. Our main goal was to propose a method to achieve root computation of a polynomial over a binary field. The main focus of this root computation is to avoid timing side-channel attacks over code-based cryptosystems. Thus, we start our literature review by searching for methods that are currently used in code-based schemes.

A direct way to find all roots of a polynomial is called exhaustive search. In summary, it just tests all possible elements and checks if it is a root. The authors in~\cite{strenzke2012fast} show an optimization to this approach. They divide the original polynomial when a root is found. This reduces the size of the polynomial and speeds up the exhaustive search in about ~30\%. Although this algorithm is efficient, it does not prevent side-channel attacks.

Berlekamp, in~\cite{berlekamp1970factoring}, proposed an alternative method to finding-roots. The classical Berlekamp Trace Algorithm computes the roots of a polynomial efficiently. The authors in~\cite{strenzke2012fast} present an optimization version by returning the recursion before the original algorithm. Another approach to compute roots was proposed in~\cite{fedorenko2002finding}, and was generalized in~\cite{Skachek2008, biswas2009}. However, all of these approaches do not present constant characteristic, and consequently are susceptible to a side-channel attack.

More recently, the authors in~\cite{petit2014finding} present a different algorithm to compute the roots of a polynomial. Lately, generalized in~\cite{petit2016finding}, the Successive Resultants Algorithm (SRA) is an efficient method, with a different approach to finding roots in a polynomial over any finite field. Nevertheless, no analysis of execution time deviation was made.

All approaches presented in this section were root-finding techniques currently in use in code-based cryptosystems. Nonetheless, it was well known that Cantor and Zassenhaus proposed one of the most efficient methods for computing roots over a polynomial in 1981~\cite{cantor1981new}. Notwithstanding its efficiency, to the best of our knowledge, it was not used in any code-based cryptosystems. For this works, we will consider the usage of the Rabin approach, which has an adaptation for fields with odd characteristics.