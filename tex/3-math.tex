This chapter explain the necessary background to understand this works. A brief mathematical and coding theory concepts, followed by a explanation of cryptography primitives used in this works and finally, a explanation of side-channel attacks.

\section{Algebraic structures}
In this section we briefly will introduce the algebraic 

\subsection{Polynomials over finite fields}
Briefly idea of polynomial over finet fields, some definitions, and what is a root.
\section{Coding theory}
Just two paragraph about the coding theory, Shanon, e some important codes.
\subsection{Linear codes}
Explain the idea of linar codes, its a code, with linear propoerties. 
\subsection{Goppa codes}
Let $m, n, t\in \mathbb{N}$. A binary Goppa code $\Gamma(L, g(z))$ is defined by a polynomial $g(z) = \sum_{i=0}^{t}g_iz^i$ over $\mathbb{F}_{2^m}$ with degree $t$ and supported by $L = (\alpha_1, \alpha_2, \dots, \alpha_n) \in \mathbb{F}_{2^m}$ with $\alpha_i \neq \alpha_j$ for $i\neq j$, such that $g(\alpha_i) \neq 0$ for all $\alpha_i \in L$ and $g(z)$ is square free. To a vector  $c = (c_1 \ldots, c_{n}) \in \mathbb{F}^n_{2}$ we associate a syndrome polynomial associate the syndrome polynomial
\begin{align}
  S_c(z) = \sum_{i=1}^{n} \frac{c_i}{z+\alpha_i},  
\end{align}
where ${z+\alpha_i}$ is invertible $\pmod{g(z)}$, i.e., $(z+\alpha_i) \times \frac{1}{z+\alpha_i} \equiv 1 \pmod{g(z)}$.
\begin{definition}
The binary Goppa code $\Gamma(L, g(z))$ consists of all vectors $c \in \mathbb{F}_{2}^n$ such that
\begin{equation}
    S_c(z) \equiv 0 \bmod{g(z)}.
\end{equation}
\end{definition}

% Change this
The parameters of a linear code are the size $n$, dimension $k$ and minimum distance $d$. We use the notation $[n,k,d]-$Goppa code for a binary Goppa code with parameters $n,k$ and $d$. If the polynomial $g(z)$ which defines a Goppa code is irreducible over $\mathbb{F}_{2^m}$, we call the code an irreducible Goppa code.

The length of a Goppa code is given by $n = |L|$ and its dimension is $k \geq n-mt$, where $t = deg(g)$, and the minimum distance of $\Gamma(L, g(z))$ is $d \geq 2t + 1$. The syndrome polynomial $S_c(z)$ can be written as:
\begin{equation}
    S_c(z) \equiv \frac{w(z)}{\sigma(z)} \mod g(z),
\end{equation}
where $\sigma(z) = {\displaystyle \prod_{i=1}^{l}(z+\alpha_i)}$ is the product over those $(z+\alpha_i)$ where there is an error in position $i$ of $c$. This polynomial $\sigma(z)$ is called Error-Locator Polynomial (ELP).

A binary Goppa code can correct a codeword $c \in \mathbb{F}_{2}^n$, obscured by an error vector $e \in \mathbb{F}_{2}^n$ with Hamming weight $w_h(e)$ up to $t$, i.e., the numbers of non-zero entries in $e$ is at most $t$. The way to correct errors is using a decoding algorithm. For irreducible binary Goppa codes we have three alternatives for that. The extended Euclidean Algorithm (EEA) and Berlekamp-Massey algorithm are out of the scope of this work, because they needed a parity-check matrix that has twice more rows then columns. The Patterson algorithm~\cite{patterson1975algebraic}, focus of this paper, can correct up to $t$ errors with a smaller structure.

\section{Cryptography primitives}
\subsection{Public-key cryptography}
\subsection{Key encapsulation mechanisms}

\section{Side-channel attacks}
In summary, a side-channel attack is any type of attack when the attacker acquire sensitive information from the implementation of the cryptosystem. There are a several types of side-channel attacks, of almost all kind of 
\subsection{Timing side-channel attacks}

A timing side-channel attack was an attack were an attacker are able to infer information from the time execution of a cryptography algorithm. To better understanding of how a timing side-channel attack works, we presents the follow example. 

\begin{example}
Let us suppose that an attacker has access to the time execution of a password validation (For instance we use an 4 digit password). This validation was made through the naive implementation presented in the Listings~\ref{lst:naive-pass}

\begin{lstlisting}[caption={Naive implementation of password check },label={lst:naive-pass},language=C]
//C
int passCheck(char * input_pass) {
    int i = 0;
    while (i < 4) {
        if (input_pass[i] == getPassAt(i)) {
            continue;
        } else {
            return -1;
        }
    }
    return 0;
}
\end{lstlisting}
So, an attacker with knowledge of the implementation, with access to the time wasted to verify a password could perform a simple attack, first  flipping the first char to all possible chars, and assume that the correct was that with spent more time. After repeat this for the all characters, the attacker recover the secret password.
\end{example}

We this toy example to give an idea of how an attacker, who knows the implementation details are able to infer information about the execution time of an algorithm. In more complex cryptosystems this inference was not simple. However, it still possible to steel sensitive information measuring the decoding execution time.

The authors in~\cite{shoufan2009timing} presents the idea of infer information on the time variance when we flip a bit over a ciphertext. This idea becomes of the fact that if we flip a correct bit (i. e. a position were a error was added) the algorithm will have less errors to correct and will return early. The opposite occurs when we flip a wrong bit. Thus, the algorithm will return lately. This attack was base of the attack presented in Chapter~\ref{ch:code-based}.


\section{Literature review}
The decoding process of McEliece cryptosystem is the most sensitive part of the algorithm. The usage of the private key demands that this algorithm does not leak any information about the key, or even, any information about the error added to the message. The authors in \cite{bucerzan2017improved} shows that a time deviation on root finding process in the Patterson's decoding algorithm could open avenues for side-channel attacks. 

The direct way to find all roots of a polynomial is called exhaustive search. In summary, it just tests all possible elements and checks if it is a root. The authors in~\cite{strenzke2012fast} show an optimization to this approach. They divide the original polynomial when a root is found. This reduces the size of the polynomial and speeds up the exhaustive search in about ~30\%. Although this algorithm is efficient, it does not prevent side-channel attacks.

An alternative proposal to find roots was created by Berlekamp in 1970. The classical Berlekamp Trace Algorithm computes the roots of a polynomial in an efficient way~\cite{berlekamp1970factoring}. It was improved in~\cite{strenzke2012fast} by stoping the recursion before the original algorithm. Another approach to compute roots was proposed in~\cite{fedorenko2002finding}, and was generalized in~\cite{Skachek2008,biswas2009}. However, all of these approaches do not present constant behavior, and consequently are susceptible to a side-channel attack.

More recently, the authors in~\cite{petit2014finding} present a different algorithm to compute the roots of a polynomial. Lately generalized in~\cite{petit2016finding}, the Successive Resultants Algorithm (SRA) is a efficient method, with a different approach to find roots in a polynomial over any finite field. Nevertheless, no analysis of execution time deviation was made.

All approaches presented in this Section was root finding techniques currently in use in code-based cryptosystems. Nonetheless, it was well know that one of the most efficiency method for computing roots over a polynomial was proposed by Cantor and Zassenhaus in 1981~\cite{cantor1981new}. Notwithstanding its efficiency, to the best of our knowledge, it wasn't used in any code-based cryptosystems. For this works, we will consider the usage of the Rabin approach, with has an adaptation for fields with odd characteristics.