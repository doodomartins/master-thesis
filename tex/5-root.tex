As argued, the leading cause of information leakage in the decoding algorithm is the process of finding the roots of the ELP. In general, the time needed to find these roots varies, often depending on the roots themselves. Thus, an attacker who has access to the decoding time can infer these roots, and hence get the vector of errors $e$. One example of this attack is presented in Section~\ref{sec:attack}, where we perform a side-channel attack over the decryption part of a cryptosystem, and we recover the secret exchange between two parts of the protocol in an acceptable time on a conventional laptop.

Factoring polynomials is a well-studied problem in finite fields area. More recently, with the rising of the code-based area, some works deal with the factorization problem on the decoding process in order to avoid timing side-channel attacks~\cite{shoufan2009timing, bucerzan2017improved}. In our literature review, we select five methods used on code-based cryptosystems to analyze and propose modifications in their implementation to make them available on cryptosystems applications.

The adjustments were made in the following algorithms to find roots: exhaustive search, linearized polynomials, Berlekamp Trace Algorithm (BTA), and Successive Resultant Algorithm (SRA). Also, we present the Cantor-Zassenhaus method, as it has a probabilistic algorithm that makes use of a random selection of polynomials in its execution. We make a security analysis over this algorithm to evaluate their blindness on a side-channel attack scenario. The description of each algorithm is given in the next sections, and the analysis is present in Chapter~\ref{ch:comparison}.

As the main focus of this works relies on the root computation of a polynomial over a finite field, we restrict our scope to polynomials over binary fields. Also, we assume that we want to compute the roots of a squarefree polynomial. Because of the nature of the ELP, since it is composed of the code locators, and each error represents on factor. However, if this it was not the nature of the polynomial, we can achieve a squarefree easily, see~\cite{von2001factoring}.

\section{Exhaustive search}
The exhaustive search, also called Chien Search~\cite{chien1964cyclic}, is a direct method, in which the evaluation of $f$ for all the elements in $\mathbb{F}_{2^m}$ is performed. A root is found when the evaluation result is zero. This method is acceptable for small fields and can be made efficient with a parallel implementation. The greatest drawback of this method is its complexity, and this is the reason it is not widely used. However, we consider it still relevant, since some NIST proposals are based on a small finite field, enabling their usage. 

Algorithm~\ref{alg:exhaustive} describes the exhaustive search. For all elements in $\mathbb{F}_{2^m}$, we evaluate the polynomial and check if it is a root or not, and this method leaks information when a root is found. This leakage occurs because whenever we found, i.e., $dummy = 0$, an extra operation is performed by adding the root found on the returning list $R$. In this way, the attacker can infer this from this additional time that a root was found, providing ways to obtain data that should be secret.

\begin{figure}[ht]
\begin{algorithm}[H]
 \KwData{$p(x)$ as univariate polynomial over $\mathbb{F}_{2^m}$ with $d$ roots, $A = [a_0, \ldots, a_{n-1}]$ as all elements in $\mathbb{F}_{2^m}$, $n$ as the length of $A$.}
 \KwResult{$R$ as a set of roots of $p(x)$.}
 $R \gets \emptyset$\;
\For{$i\gets0$ \KwTo $n-1$}{
    $dummy \gets p(A[i])$\;
   \If{$dummy == 0$}{
        $R.add(A[i])$\;
    }
}
\Return $R$\;
  \caption{Exhaustive search algorithm for finding roots of a univariate polynomial over $\mathbb{F}_{2^m}$.}\label{alg:exhaustive}
\end{algorithm}
\end{figure}

One solution to avoid this leakage is to permute the elements of vector $A$ before the evaluation. Using this technique, an attacker can identify the extra operation, but without learning any secret information. In our case, we use the Fisher-Yates shuffle~\cite{black2005fisher} for shuffling the elements of vector $A$. In~\cite{wang2018fpga}, the authors show an implementation of the shuffling algorithm which is safe against timing attacks. Thus, we can build and exhaustive search without leaking information against a side-channel attack. The line 1 on Algorithm~\ref{alg:exhaustive_permuted} shows the permutation call of the elements and the computation of the roots.

\begin{figure}[ht]
\begin{algorithm}[H]
 \KwData{$p(x)$ as a univariate polynomial over $\mathbb{F}_{2^m}$ with $d$ roots, $A = [a_0, \ldots, a_{n-1}]$ as all elements in $\mathbb{F}_{2^m}$, $n$ as the length of $A$.}
 \KwResult{$R$ as a set of roots of $p(x)$.}
  permute$(A)$\;
 $R \gets \emptyset$\;
\For{$i\gets0$ \KwTo $n-1$}{
    $dummy \gets p(A[i])$\;
   \If{$dummy == 0$}{
        $R.add(A[i])$\;
    }
}
\Return $R$\;
 \caption{Exhaustive search algorithm with a countermeasure for finding roots of an univariate polynomial over $\mathbb{F}_{2^m}$.}
  \label{alg:exhaustive_permuted}
\end{algorithm}
\end{figure}

Using this approach, we add one extra step to the algorithm. However, this permutation blurs the sensitive information of the algorithm, making the usage of Algorithm~\ref{alg:exhaustive_permuted} slightly harder for the attacker to acquire timing leakage. In this case, we want to avoid the addition of an \texttt{else} clause adding the elements that are not a root in another list. This addition will absolutely reduce the time variance on the execution of the algorithm, but it could facilitate cache attacks~\cite{dan2005}.

\section{Berlekamp Trace Algorithm}
Our second strategy is the classical Berlekamp factoring algorithm~\cite{berlekamp1970factoring}. Berlekamp presents an efficient algorithm to factor a polynomial, which can be used to find its roots. We call this algorithm \emph{Berlekamp Trace Algorithm} since it relies on the trace function properties to split a polynomial into two small polynomials. This classical algorithm is one of the most used on code-based cryptosystems for the reason that it has lower complexity when compared with exhaustive search~\cite{biswas2009efficient}. 

The trace function is defined as $Tr(x) = x + x^{2} + x^{2^{2}} + \dots + x^{2^{m-1}}$. It is possible to change BTA for finding roots of a polynomial $p(x)$ using $\beta = \{\beta_1, \beta_2, \ldots, \beta_m\}$ as a standard basis of $\mathbb{F}_{2^{m}}$, and then computing the greatest common divisor between $p(x)$ and $Tr(\beta_0 \cdot x)$. After that, BTA performs two recursive calls; one with the result of gcd algorithm and the other with the remainder of the division $p(x) / \gcd(p(x), Tr(\beta_i \cdot x))$. On the next call, the BTA must use a different basis from the previous one. 

The base case is when the degree of the input polynomial is smaller than one. In this case, BTA returns the root by getting the independent term of the polynomial. In summary, it is a divide and conquer like algorithm since it splits the task of computing the roots of a polynomial $p(x)$ into the roots of two smaller polynomials. Some improvements can be made on BTA with a hybrid version, i.e. when the degree of the polynomial is two, we can use a different algorithm to find its factors, as presented in~\cite{strenzke2012fast}. As efficiency was not the focus of this work, we do not consider this approach. All steps of the Berlekamp trace algorithm are described in Algorithm~\ref{alg:bta}.


\begin{figure}[ht]
\begin{algorithm}[H]
 \KwData{$p(x)$ as a univariate polynomial over $\mathbb{F}_{2^m}$ and i to select the basis.}
 \KwResult{The set of roots of $p(x)$.}
    \If{$deg(p(x)) \leq 1$}{
        \Return root of $p(x)$\;
    }
    $p_{0}(x) \gets gcd(p(x), Tr(\beta_{i}\cdot x))$\;
    $p_{1}(x) \gets p(x) / p_{0}(x)$ \;
\Return $BTA(p_{0}(x), i + 1) \cup BTA(p_{1}(x), i + 1)$\;
 \caption{Berlekamp Trace Algorithm -- $BTA(p(x), i)-rf$.}
  \label{alg:bta}
\end{algorithm}
\end{figure}

As we can see, a direct implementation of Algorithm~\ref{alg:bta} has no constant execution time. The recursive behavior may leak information about the characteristics of roots in a side-channel attack. Additionally, in our experiments, we note that the behavior of the gcd with the trace function might result in a polynomial with the same degree. Therefore, BTA will divide this input polynomial in a future call upon a different basis. Consequently, there is no guarantee of a constant number of executions because we cannot control if a polynomial will be split or not. 

In order to avoid the nonconstant number of executions and avoid timing side-channel attacks, we propose an iterative implementation of Algorithm~\ref{alg:bta}, here after referred to $BTA-it$. In this way, our proposal iterates in a fixed number of iterations instead of calling itself until the base case. The main idea is not changed; we still divide the task of computing the roots of a polynomial $p(x)$ into two smaller instances. However, we change the approach of the division of the polynomial. Since we want to compute the same number of operations independent of the degree of the polynomial, we perform the gcd with a trace function for all basis in $\beta$, and choose a division that results in two new polynomials with a medium degree.

This new approach allows us to define a fixed number of iterations for our version of BTA. Since we always divide into two small instances, we need $t-1$ iterations to split a polynomial of degree $t$ to $t$ polynomials of degree $1$. Then we just need to add a stack to control the polynomials to be divided. Algorithm~\ref{alg:ibta} presents our new approach of the iterative $BTA-it$.

\begin{figure}[ht]
\begin{algorithm}[H]
 \KwData{$p(x)$ as an univariate polynomial over $\mathbb{F}_{2^m}$, $t$ as number of expected roots.}
 \KwResult{The set of roots of $p(x)$.}
    $g \gets \{p(x)\};$ \tcp{The set of polynomials to be computed}
    \For{$k \gets 0$ \KwTo $t$}{
        $current = g.pop()$\;
        Compute $candidates$ $=$ $gcd(current, Tr(\beta_{i}\cdot x))$ $\forall$ $\beta_{i}$ $\in$ $\beta$\;
        Select $p_{0}$ $\in$ $candidates$ such as $p_{0}.degree$ $\simeq$ $\frac{current}{2}$\;
        $p_{1}(x) \gets current / p_{0}(x)$ \;
        \If{$p_{0}.degree == 1$}{
            $R.add($root of $p_{0})$
        } \Else{
            $g.add(p_{0})$\;
        }
        \If{$p_{1}.degree == 1$}{
            $R.add($root of $p_{1})$
        } \Else{
            $g.add(p_{1})$\;
        }
    }
    \Return{$R\;$}
 \caption{Iterative Berlekamp Trace Algorithm -- $BTA(p(x))-it$.}
  \label{alg:ibta}
\end{algorithm}
\end{figure}

Algorithm~\ref{alg:ibta} extracts a root of the polynomial when the variable $current$ has a polynomial with degree one. If this degree is higher than one, then the algorithm needs to continue dividing the polynomial until it finds a root. The algorithm does that by adding the polynomial in a stack and reusing this polynomial in a future division. The iterative BTA has a higher complexity when compared to the original approach. Nevertheless, with our new approach, we achieve a more constant time execution on the root-finding task, consequently reducing the knowledge obtained by an attacker. In Chapter~\ref{ch:comparison}, we analyse and compare our iterative BTA with other approaches.

\section{Linearized Polynomials}
The third countermeasure proposed is based on linearized polynomials. The authors in \cite{fedorenko2002finding} propose a method to compute the roots of a polynomial over $\mathbb{F}_{2^m}$, using a particular class of polynomials, called linearized polynomials. In \cite{strenzke2012fast}, this approach is a recursive algorithm which the author calls ``\texttt{dcmp-rf}''. In our solution, however, we present an iterative algorithm. First, we define linearized polynomials as follows:

\begin{definition}
A polynomial $\ell(y)$ over $\mathbb{F}_{2^m}$ is a linearized polynomial if
\begin{equation}
    \ell(y) = \sum_i c_iy^{2^i},
\end{equation}
where $c_i \in \mathbb{F}_{2^m}$.
\end{definition}
In addition, from~\cite{truong2001fast}, we have Lemma~\ref{lemma:lin} that describes the main property of linearized polynomials for finding roots.
\begin{lemma}
\label{lemma:lin}
    Let $y \in \mathbb{F}_{2^m}$ and let $\alpha^0, \alpha^1, \ldots, \alpha^{m-1}$ be a standard basis over $\mathbb{F}_2$. If
    \begin{equation}
        y = \sum_{k=0}^{m-1} y_k\alpha^k,\quad y_k \in \mathbb{F}_2
    \end{equation}
    and $\ell(y) =\sum_j c_jy^{2^j}$, then
      \begin{equation}
        \ell(y) = \sum_{k=0}^{m-1} y_k\ell(\alpha^k).
    \end{equation}
\end{lemma}

We call $A(y)$ over $\mathbb{F}_{2^m}$ an affine polynomial if $A(y) = \ell(y) + \beta$ for $\beta \in \mathbb{F}_{2^m}$, where $\ell(y)$ is a linearized polynomial. Using this definitions, we can construct a method for finding the ELP roots. First, we present a toy example from~\cite{truong2001fast} to understand the idea behind finding roots using linearized polynomials.

\begin{example}\label{ex:1}
Let us consider the polynomial $f(y) = y^2 + (\alpha^2+1)y + (\alpha^2 +\alpha +1)y^0$ over $\mathbb{F}_{2^3}$ and $\alpha$ a primitive element in $\mathbb{F}_2[x]/ x^3+x^2+1$. Since we are trying to find roots, we can write $f(y)$ equals to zero
 $$ y^2 + (\alpha^2+1)y + (\alpha^2 +\alpha +1)y^0 = 0$$
 and after that, we can rewrite the sentence like
\begin{equation}\label{eq:example_1}
    y^2 + (\alpha^2+1)y   = (\alpha^2 +\alpha +1)y^0.
\end{equation}
We can point that on the left hand side of Equation~\ref{eq:example_1}, $\ell(y) = y^2 + (\alpha^2+1)y$ is a linearized polynomial over $\mathbb{F}_{2^3}$ and Equation~\ref{eq:example_1} can be expressed as
\begin{equation}\label{eq:example_1_2}
    \ell(y) = \alpha^2 +\alpha +1.
\end{equation}
If $y = y_2\alpha^2 + y_1\alpha + y_0 \in \mathbb{F}_{2^3}$ then, according to Lemma~\ref{lemma:lin}, Equation~\ref{eq:example_1_2} becomes
\begin{equation}\label{eq:example_1_3}
    y_2\ell(\alpha^2) + y_1\ell(\alpha) + y_0\ell(\alpha^0) = \alpha^2 +\alpha +1.
\end{equation}
We can compute $\ell(\alpha^0),\ell(\alpha)$ and $\ell(\alpha^2)$ using the left hand side of Equation~\ref{eq:example_1} and we have the following values
\begin{equation}\label{eq:example_1_4}
    \begin{split}
        \ell(\alpha^0) &= (\alpha^0)^2 + (\alpha^2+1)(\alpha^0) = \alpha^2+1 + 1 = \alpha^2, \\
        \ell(\alpha) &= (\alpha)^2 + (\alpha^2+1)(\alpha) = \alpha^2 + \alpha^2+ \alpha + 1 = \alpha + 1, \\
        \ell(\alpha^2) &= (\alpha^2)^2 + (\alpha^2+1)(\alpha^2) = \alpha^4 +\alpha^4 +  \alpha^2 = \alpha^2.
    \end{split}
\end{equation}
A substitution of Equation~\ref{eq:example_1_4} into Equation~\ref{eq:example_1_3} gives us
\begin{equation}\label{eq:example_1_5}
     (y_2+y_0)\alpha^2 + (y_1)\alpha + (y_1)\alpha^0 = \alpha^2 +\alpha +1.
\end{equation}
Equation~\ref{eq:example_1_5} can be expressed as a matrix in the form
\begin{equation}\label{eq:example_1_6}
    \begin{bmatrix} y_2 & y_1 & y_0 \end{bmatrix}
    \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 1 \\
    1 & 0 & 0
    \end{bmatrix}
    =
    \begin{bmatrix} 1 & 1 & 1 \end{bmatrix}.
\end{equation}
If one solves simultaneously the linear system in Equation~\ref{eq:example_1_6} then the results are the roots of the original polynomial given in Equation~\ref{eq:example_1}. From Equation~\ref{eq:example_1_5}, one observes that the solutions are $y=110$ and $y=011$. Furthermore, we can translate $110$ and $011$ to $\alpha + 1$ and $\alpha^2 + \alpha$. After all this steps, it is easy to check if we found the factors of $f(y)$. We just need to multiply $y - \alpha + 1$ and $y- \alpha^2 + \alpha$, thus we get

\begin{equation*}\label{eq:example_1_7}
    (y - \alpha + 1) (y - \alpha^2 + \alpha) = y^2 + (\alpha^2+1)y + (\alpha^2 +\alpha +1) = f(y).
\end{equation*}
\end{example}

Fortunately, the authors in \cite{fedorenko2002finding} provide a generic decomposition for finding affine polynomials. Thus we can transform the error locator polynomial in an affine polynomial. In their work, each polynomial in the form $F(y) = \sum_{j=0}^{t} f_jy^j$ for $f_j \in \mathbb{F}_{2^m}$ can be represented as
\begin{equation}
\label{eq:f_y}
    F(y) = f_3y^3 + \sum_{i=0}^{\lceil (t-4)/5 \rceil} y^{5i}(f_{5i} + \sum_{j=0}^{3} f_{5i+2^j}y^{2^j}).
\end{equation}

After that, we can summarize all the steps on Algorithm~\ref{alg:linearized}. The function on Step 21, ``\texttt{generate($m$)}'', refers to the generation of the elements in $\mathbb{F}_{2^m}$ using Gray codes, see \cite{savage1997survey} for more details about Gray codes. The linearized method differs from the exhaustive search on the evaluation process; it is much more efficient to compute this evaluation because the polynomial is in the linearized form. Algorithm~\ref{alg:linearized} presents our countermeasure in the last steps of the algorithm, i.e., we added a dummy operation for blinding if $X[j]$ is a root of polynomial $F(x)$. The analysis of the linearized method and its countermeasure are presented in Chapter~\ref{ch:comparison}.

\begin{figure}
\begin{algorithm}[H]
 \KwData{$F(x)$ as a univariate polynomial over $\mathbb{F}_{2^m}$ with degree $t$ and $m$ as the extension field degree.}
 \KwResult{$R$ as a set of roots of $p(x)$.}
 $\ell^k_i \gets \emptyset$; $\ell_{is} \gets \emptyset$; $A^j_k \gets \emptyset$; $R \gets \emptyset$; $dummy \gets \emptyset$\;
 \If{$f_0  == 0$}{
 $R.append(0)$\;
 }
 \For{$i\leftarrow 0$ \KwTo $\lceil (t-4)/5 \rceil$}
 {
    $\ell_i(x) \gets 0$\;
    \For{$j\gets 0$ \KwTo $3$}{
      $\ell_i(x) \gets \ell_i(x) + f_{5i+2^j}x^{2^j}$\;
      }
    $\ell_{is}[i] \gets \ell_i(x)$\;
 }
 \For{$k\gets 0$ \KwTo $m-1$}{
    \For{$i\leftarrow 0$ \KwTo $\lceil (t-4)/5 \rceil$}
    {
        $\ell^k_i \gets \ell_{is}(\alpha^k)$\;
    }

 }
 $A^0_i \gets \emptyset$\;
 \For{$i\gets 0$ \KwTo $\lceil (t-4)/5 \rceil$}{
  $A^0_i \gets f_{5i}$\;
 }

 $X \gets \text{generate}(m)$\;
 \For{$j\gets 1$ \KwTo $2^m - 1$}{
    \For{$i\gets 0$ \KwTo $\lceil (t-4)/5 \rceil$}{
        $A \gets A^{j-1}_i$\;
        $A \gets A + \ell^{\delta(X[j], X[j-1])}_i$\;
        $A^j_i \gets A$\;
    }
 }
\For{$j\gets 1$ \KwTo $2^m - 1$}{
    $result \gets 0$\;
    \For{$i\gets 0$ \KwTo $\lceil (t-4)/5 \rceil$}{
        $result = result + (X[j])^{5i}A^j_i$\;
    }
    $eval = result + f_3(X[j])^{3}$\;
    \eIf{$eval == 0$}{
        $R.append(X[j])$\;
    }{
        $dummy.append(X[j])$\;
    }
}
\Return $R$\;
 \caption{Linearized polynomials for finding roots over $\mathbb{F}_{2^m}$.}
  \label{alg:linearized}
\end{algorithm}
\end{figure}

\section{Successive Resultant Algorithm}
In \cite{petit2014finding}, the authors present an alternative method for finding roots in $\mathbb{F}_{p^m}$. Later on, the authors better explain the method in~\cite{petit2016finding}. The Successive Resultant Algorithm (SRA) relies on the fact that it is possible to find roots exploiting properties of an ordered set of rational mappings.

Given a polynomial $f$ of degree $d$ and a sequence of rational maps $K_1,\ldots, K_t$, the algorithm computes finite sequences of length $j \leq t+1$ obtained by successively transforming the roots of $f$ by applying the rational maps. The algorithm is as follows: Let $\{v_1,\ldots,v_m\}$ be an arbitrary basis of $\mathbb{F}_{p^m}$ over $\mathbb{F}_p$, then it is possible to define $m+1$ functions $\ell_0, \ell_1,\ldots, \ell_m$ from $\mathbb{F}_{p^m}$ to $\mathbb{F}_{p^m}$ such that
$$
\left \{
\begin{array}{l}
     \ell_0(z) = z\\
     \ell_1(z) = \prod_{i\in \mathbb{F}_p}\ell_0(z-iv_1)\\
     \ell_2(z) = \prod_{i\in \mathbb{F}_p}\ell_1(z-iv_2)\\
     \vdots \\
     \ell_m(z) = \prod_{i\in \mathbb{F}_p}\ell_{m-1}(z-iv_m).\\
\end{array}
\right.
$$
The functions $\ell_j$ are examples of linearized polynomials, as previously defined. Our next step is to present the theorems from \cite{petit2014finding}. We suggest the reader to consult the original work for proofs and more details.
\begin{theorem}\label{lemma_1}
\begin{itemize}
    \item[a)] Each polynomial $\ell_i$ is split and its roots are all elements of the vector space generated by $\{v_1, \ldots,v_i\}$. In particular, we have $\ell_n(z) = z^{p^m} -z$.
    \item[b)] We have $\ell_i(z)  = \ell_{i-1}(z)^p - a_i\ell_{i-1}(z)$ where $a := (\ell_{i-1}(v_i))^{p-1}$.
    \item[c)] If we identify $\mathbb{F}_{p^m}$ with the vector space $(\mathbb{F}_p)^m$, then each $\ell_i$ is a $p$-to-$1$ linear map of $\ell_{i-1}(z)$ and a $p^i$ to $1$ linear map of $z$.
\end{itemize}
\end{theorem}

From Theorem~\ref{lemma_1} and its properties, we can reach the following polynomial system:
\begin{equation}\label{eq:system_1}
    \left \{
\begin{array}{l}
    f(x_1) = 0\\
     x_j^p = a_jx_j = x_{j+1} \quad j=1,\ldots, m-1\\
     x_n^p - a_nx_n = 0
\end{array}
\right.
\end{equation}
where the $a_i \in \mathbb{F}_{p^n}$ are defined as in Theorem~\ref{lemma_1}. Any solution of this system provides us with a root of $f$ by the first equation, and the $n$ last equations together imply this root belongs to $\mathbb{F}_{p^n}$. From this system of equations,~\cite{petit2014finding} derives Theorem~\ref{lemma_2}.

\begin{theorem}\label{lemma_2}
Let $(x_1,x_2,\ldots,x_m)$ be a solution of the equations in Equation~\ref{eq:system_1}. Then $x_1 \in \mathbb{F}_{p^m}$ is a solution of $f$. Conversely, given a solution $x_1 \in \mathbb{F}_{p^m}$ of $f$, we can reconstruct a solution of all equations in Equation~\ref{eq:system_1} by setting $x_2 =x_1^p - a_1x_1$, etc.
\end{theorem}

In \cite{petit2014finding}, the author present an algorithm for solving the system in Equation~\ref{eq:system_1} using resultants. The solutions of the system are the roots of polynomial $f(x)$. It is worth remarking that this algorithm is almost constant-time, and hence we just need to protect the branches presented on it.



% \begin{figure}[ht]
% \begin{algorithm}[H]
%  \KwData{$f(x)$ as an univariate polynomial over $\mathbb{F}_{2^m}$, $t$ as number of expected roots.}
%  \KwResult{The set of roots of $p(x)$.}
%     $Factors \gets \{f\};$ \tcp{The set of polynomials to be computed}
%     $k \gets 1$\;
%     $it_{max} \gets 2 \lceil\log{\frac{t^2}{\epsilon}}\rceil$\;
%     \While{$k < it_{max}$}{
%         $h \xleftarrow{\$} \mathbb{F}_{2^m}$ \tcp{choose $h$ with degree $< t$ at random}
%         $g \gets gcd(h, f)$\;
%         \If{$g = 1$}{
%             $g \gets tr(h)$
%         }
%         \For{$fact \in Factors \quad$and$\quad fact.degree > d$}{
%             \If{$gcd(g, fact) \neq 1 \quad$ and $\quad gcd(g, u) \neq u$}{
%                 $Factors.remove(u)$\;
%                 $Factors.insert(gcd(g,u))$\;
%                 $Factors.insert(u/gcd(g,u))$\;
%             }
%         }
%         \If{$p_{1}.degree == 1$}{
%             $R.add($root of $p_{1})$
%         }
%     }
%     \Return{$Factors$}
%  \caption{Successive Resultant Algorithm -- $SRA(p(x))$.}
%   \label{alg:sra}
% \end{algorithm}
% \end{figure}


\section{Rabin Algorithm}
Our last proposal for finding the roots of the error locator polynomial was the classical method proposed by Rabin~\cite{rabin1980probabilistic}. This classical method is a probabilistic algorithm and has running time similar to the Cantor-Zassenhaus algorithm~\cite{cantor1981new}. This method is used to factor huge polynomials, with a higher degree and over larger fields. Although the main focus of this works relies on polynomials with no more than 200 roots and fields of size at most $2^{18}$, we consider this algorithm because of its efficiency.

The Cantor-Zassenhaus differ from Rabin's method on the field extension of the polynomial to be factored. Since Cantor's method is designed only for odd extensions, we do not consider them in our works. However, Rabin made an adaptation to support even fields, introducing a trace computation to finding a nontrivial factorization of the input polynomial, similar to the BTA method. This addition increases the execution time of the algorithm, but it is still is efficient, even when used in large field extensions. 

The main difference between Cantor-Zassenhaus and Rabin's method to the other methods presented in this chapter is the insertion of a randomness choice of an element in the algorithm. The computation of the roots depends on the choice of a random element in $\mathbb{F}_{2^{m}}$. This random selection was used with a trace function and a $gcd$ to find a nontrivial factorization of the original polynomial. This class of algorithm are also called probabilistic algorithms.

Rabin method is a probabilistic algorithm because of its random behavior, because of the chance o failure, the parameter $\epsilon$ is used to define the maximum number of iterations on the algorithm. Since we always choose a random element to continue, the probability of failure of the algorithm is based on the fact that a random selection could not result in a nontrivial factor of the input polynomial. However, Ben-Or proves that this probability is $1 - 1/2^{t-1}$, where $t$ is the degree of the polynomial~\cite{ben1981probabilistic}.

To the best of our knowledge, any code-based cryptosystem makes use of a probabilistic algorithm as a method for root finding. The original proposal was designed as a recursive algorithm, here we present an iterative version, as show in~\cite{von2001factoring}. Algorithm \ref{alg:new} shows all steps to compute the roots. 


\begin{figure}[ht]
\begin{algorithm}[H]
 \KwData{$f(x)$ as an univariate polynomial over $\mathbb{F}_{2^m}$, $t$ as number of expected roots.}
 \KwResult{The set of roots of $p(x)$.}
    $Factors \gets \{f\};$ \tcp{The set of polynomials to be computed}
    $k \gets 1$\;
    $it_{max} \gets 2 \lceil\log{\frac{t^2}{\epsilon}}\rceil$\;
    \While{$k < it_{max}$}{
        $h \xleftarrow{\$} \mathbb{F}_{2^m}$ \tcp{choose $h$ with degree $< t$ at random}
        $g \gets gcd(h, f)$\;
        \If{$g = 1$}{
            $g \gets tr(h)$
        }
        \For{$fact \in Factors \quad$and$\quad fact.degree > d$}{
            \If{$gcd(g, fact) \neq 1 \quad$ and $\quad gcd(g, u) \neq u$}{
                $Factors.remove(u)$\;
                $Factors.insert(gcd(g,u))$\;
                $Factors.insert(u/gcd(g,u))$\;
            }
        }
        \If{$p_{1}.degree == 1$}{
            $R.add($root of $p_{1})$
        }
    }
    \Return{$Factors$}
 \caption{Probabilistic root finding algorithm -- $Rabin(p(x))$.}
  \label{alg:new}
\end{algorithm}
\end{figure}

Rabin's algorithm is an equal degree factorization method. Hence we are computing the roots of a polynomial that is composed only by linear factors. We simplified a step where the original method computes before the trace $g = \sum_{i=0}^{d-1} h^{q^{i}}$, where $d$ is equal to the degree of the factors of $f$. 

Rabin's algorithm it is a great example of a naturally safe against timing side-channel atack. Since it uses a random selection on their execution, we consider that this randomness does not permit an attacker to infer any information about the roots. Moreover, since we are using a random selection, if an attacker could not infer any information from the time variance of the algorithm. A more detailed analysis was presented in Chapter~\ref{ch:comparison}.
